---
layout: exercise
title: Learning (deep) continuous functions - readings
description: Functional hypothesis spaces and deep probabilistic models
---

Here are a selection of classic and recent readings on neural network models:

Check out [Rumelhart et al. (1986)](https://www.nature.com/articles/323533a0) on the origins of the backprop algorithm as well as eary work examining representation learning in autoencoders [Rumelhart & Todd (1991)](https://web.stanford.edu/class/psych209a/ReadingsByDate/02_08/RumelhartTodd93.pdf)

In the chapter we describe a deep generative model. More recent work [Kingma & Welling (2013)](https://arxiv.org/abs/1312.6114) presents an implementation called Variational Autoencoders (VAE).

See even more recent work examining VAE models that can incorporate multiple modalities [Wu & Goodman (2018)](https://arxiv.org/abs/1802.05335).